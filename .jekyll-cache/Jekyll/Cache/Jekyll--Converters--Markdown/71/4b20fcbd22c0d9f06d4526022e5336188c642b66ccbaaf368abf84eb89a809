I"8<p>이번 포스팅에서는 object detection의 기본 개념과 RCNN, Fast RCNN, Faster RCNN, YOLO에 대해서 알아보고자 합니다.</p>

<hr />

<h1 id="object-detection이란">Object Detection이란?</h1>

<p>object detection은 사진이나 영상과 같은 디지털 이미지에서 <code class="language-plaintext highlighter-rouge">물체를 찾아내고</code> 그 <code class="language-plaintext highlighter-rouge">물체가 어떤 것</code>(예를 들면, 인간, 동물, 차 등)인지 판별하는 작업입니다. 최종 목적은 <strong>“어떤 물체가 어디에 있는지”</strong>입니다. 즉 2가지의 task를 해결해야 합니다.</p>

<hr />

<h1 id="object-detection-주요-알고리즘">Object Detection 주요 알고리즘</h1>

<p>딥러닝 기반의 object detection 알고리즘은 2-stage detector와 1-stage detector 두가지로 나뉩니다.</p>

<p>Two-stage detector는 deep feature를 이용하여 <code class="language-plaintext highlighter-rouge">1. 물체의 영역을 찾아내고</code> <code class="language-plaintext highlighter-rouge">2. 분류합니다.</code> 이 방법은 높은 detection 정확도를 가지지만 이미지 별로 추론 단계를 거치기 때문에 일반적으로 <code class="language-plaintext highlighter-rouge">속도가 느립니다.</code></p>

<p>One-stage detector는 영역을 제안하지 않고 이미지 위의 bounding box를 예측합니다. 이 알고리즘은 two-stage detector보다 속도가 빠르고 구조적으로 단순하다는 장점이 있지만 불규칙한 모양의 물체나 작은 물체들을 인식하는 task에서는 <code class="language-plaintext highlighter-rouge">정확도가 떨어집니다.</code></p>

<ul>
  <li>Two-stage object detection algorithms
    <ol>
      <li>RCNN and SPPNet (2014)</li>
      <li>Fast RCNN and Faster RCNN (2015)</li>
      <li>Mask R-CNN (2017)</li>
      <li>Pyramid Networks/FPN (2017)</li>
      <li>G-RCNN (2021)</li>
    </ol>
  </li>
  <li>One-stage object detection alogorithms
    <ol>
      <li>YOLO (2016)</li>
      <li>SSD (2016)</li>
      <li>RetinaNet (2017)</li>
      <li>YOLOv3 (2018)</li>
      <li>YOLOv4 (2020)</li>
      <li>YOLOR (2021)</li>
    </ol>
  </li>
</ul>

<p><strong>Object detection은 2가지 문제를 해결해야 합니다.</strong></p>

<ol>
  <li>임의의 물체 갯수를 찾아내기</li>
  <li>각 물체들이 어떤 건지 분류하고 bounding box 사이즈를 평가하기</li>
</ol>

<hr />

<h1 id="rcnn">RCNN</h1>

<p><img src="/assets/img/post-img/object-detection/RCNN.png" width="700px" /></p>

<ol>
  <li>이미지를 input으로 집어 넣음</li>
  <li>selective search 알고리즘을 통해 ROI(약 2000개)를 추출하고 잘라냄(*selectvie search: 객체와 주변간의 색감, 질감차이, 다른 물체에 애워쌓여있는지 여부 등을 파악해서 다양한 전략으로 물체의 위치를 파악하는 알고리즘)</li>
  <li>227x227 사이즈로 warping함</li>
  <li>warped image를 각각 CNN 모델에 집어 넣고 feature vector를 추출함</li>
  <li>각각 binary SVM classification, bounding box regression을 진행하여 결과를 도출함</li>
</ol>

<hr />

<h1 id="fast-rcnn">Fast RCNN</h1>

<p><img src="/assets/img/post-img/object-detection/FastRCNN.png" width="700px" /></p>

<p>아래의 RCNN 단점들을 보완한 모델이며 이름에서 알 수 있듯이 속도가 빨라진 모델입니다.</p>
<ul>
  <li>227x227 사이즈로 warping 시키면서 성능 손실이 발생함</li>
  <li>selective search를 통해 2000개의 이미지 proposal 후보를 모두 CNN 모델에 집어 넣으면서 시간이 매우 오래 걸림</li>
  <li>selective search나 SVM이 GPU를 사용하기엔 적합한 구조가 아님</li>
  <li>뒷 부분에서 수행한 computation을 share하지 않음</li>
</ul>

<ol>
  <li>selective search를 통해 region proposal을 뽑아냄</li>
  <li>crop regrion을 그대로 가지고 있는채로 전체 이미지를 CNN 모델에 집어 넣음</li>
  <li>ROI를 feature map 크기에 맞춰서 projection함</li>
  <li>ROI pooling함(FC에 넣을 수 있게 같은 resolution의 feature map으로 바꿔줌)
 ROI pooling: 각각의 영역 크기가 비슷하게 section을 나눈 후 max pooling함</li>
</ol>

<p><img src="/assets/img/post-img/object-detection/ROIpooling.png" width="300px" /></p>

<ol>
  <li>fixed length feature vector를 FCs에 집어 넣은 후 두 ouput layer로 나눔</li>
  <li>Softmax를 이용한 Classfication과 Bounding box regression을 진행함</li>
</ol>

<h2 id="fast-rcnn의-loss-function">Fast RCNN의 Loss function</h2>

<p><img src="/assets/img/post-img/object-detection/FastRCNN-loss.png" width="500px" /></p>

<p>classfication은 log loss, bouding box regression은 smooth L1 loss를 사용합니다.</p>

<hr />

<h1 id="faster-rcnn">Faster RCNN</h1>

<p><img src="/assets/img/post-img/object-detection/FasterRCNN.png" width="700px" /></p>

<p>region proposal을 GPU를 통해 학습합니다. Fast RCNN에서 selective search가 했던 일을 Faster RCNN에서 RPN이 대신 하고 나머지는 Fast RCNN과 동일합니다.</p>
<ol>
  <li>VGG net을 이용하여 feature map 추출</li>
  <li>RPN 네트워크로 물체가 있을 법한 위치를 찾음(RPN 구조는 아래 그림 참고)
<img src="/assets/img/post-img/object-detection/The-structure-of-RPN.png" width="700px" /></li>
  <li>k개의 anchor box 사용(다양한 크기의 bounding box)</li>
  <li>sliding window를 거쳐 각 위치에 대해 regression(물체가 있는지 없는지)과 classification을 수행함</li>
</ol>

<hr />

<h1 id="yolo">YOLO</h1>

<p><img src="/assets/img/post-img/object-detection/YOLO.png" width="700px" /></p>

<p>YOLO는 위의 모델들과 다르게 One-stage object detection alogorithm입니다. YOLO는 아래의 특징들을 가지고 있습니다.</p>
<ul>
  <li>객체 검출을 하나의 회귀 문제로 보기 때문에 매우 빠름</li>
  <li>예측을 할 때 이미지 전체를 처리하기 때문에 background error가 작음</li>
  <li>훈련 단계에서 보지 못한 새로운 이미지에 대해서도 검출 정확도가 높음</li>
  <li>SOTA 객체 검출 모델에 비해 정확도(mAP)가 다소 떨어짐</li>
</ul>

<ol>
  <li>input image를 S x S 그리드로 나눔. 어떤 객체의 중심이 특정 그리드 셀 안에 위치한다면 그 그리드 셀이 해당 객체를 검출해야 함</li>
  <li>각각의 그리드 셀은 B개의 bouding box와 그 bounding box의 confidence score를 예측함
    <ul>
      <li>bounding box: 정규화된 x, y, w, h</li>
      <li>confidence score: 객체 포함 확률 * bouding box 정확도</li>
    </ul>
  </li>
  <li>각각의 그리드 셀은 conditional class probabilities=$Pr(Class_i|Object)$ 예측
    <ul>
      <li>객체가 있다는 조건 하에 어떤 클래스인지 조건부 확률</li>
      <li>B의 개수와는 무관하게 하나의 그리드 셀에서는 클래스 하나만 예측
<img src="/assets/img/post-img/object-detection/YOLOdetail.png" width="700px" /></li>
    </ul>
  </li>
  <li>마지막 계층에는 linear activation function을 적용, 나머지 모든 계층에는 leaky ReLU 적용</li>
</ol>

<h2 id="yolo의-loss-function">YOLO의 loss function</h2>
<p>최적화를 쉽게 하기 위해 sum-squared error 기반입니다.</p>
<ul>
  <li>localization loss: bounding box 위치 예측</li>
  <li>classification loss: 클래스 예측
YOLO의 문제점을 보완하기 위해 loss를 아래처럼 개선하여 사용합니다.</li>
  <li>문제점1: 배경 영역이 객체가 없는 영역보다 더 크기 때문에 불균형을 초래함
    <ul>
      <li>개선법: localization loss의 가중치를 증가시키고 그 중에서 객체가 없는 그리드 셀의 confidence loss보다 객체가 있는 confidence loss의 가중치를 증가시킴($λ_{coord}$와 $λ_{noobj}$로 조절)</li>
    </ul>
  </li>
  <li>문제점2: 큰 bounding box와 작은 bouding box에 대해 동일한 가중치로 loss를 계산함(작은 bouding box는 위치 변화에 더 민감함)
    <ul>
      <li>개선법: width와 height에 square root를 취해줌</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/img/post-img/object-detection/YOLOloss.png" width="700px" /></p>

<ul>
  <li>$1_i^obj$: 그리드 셀 i 안에 객체가 존재하는지 여부
    <ul>
      <li>객체가 존재하면 1, 존재하지 않으면 0</li>
    </ul>
  </li>
  <li>$1_ij^obj$: 그리드 셀 i의 j번째 bounding box predictor가 사용되는지 여부</li>
</ul>

<ol>
  <li>Object가 존재하는 그리드 셀 i의 bouding box predictor j에 대해, x와 y의 loss를 계산</li>
  <li>Object가 존재하는 그리드 셀 i의 bounding box predictor j에 대해, w와 h의 loss를 계산</li>
  <li>Object가 존재하는 그리드 셀 i의 bounding box predictor j에 대해, confidence score의 loss를 계산 ($C_i = 1$)</li>
  <li>Object가 존재하지 않는 그리드 셀 i의 bounding box predictor j에 대해, confidence score의 loss를 계산. ($C_i = 0$)</li>
  <li>Object가 존재하는 그리드 셀 i에 대해, conditional class probability의 loss를 계산. ($p_i(c)=1$ if class c is correct, otherwise: $p_i(c)=0$)</li>
</ol>

<ul>
  <li>$λ_{coord}$: coordinates(x, y, w, h)에 대한 loss와 다른 loss들과의 균형을 위한 balancing parameter.</li>
  <li>$λ_{noobj}$: 객체가 있는 box와 없는 box 간에 균형을 위한 balancing parameter. (일반적으로 image내에는 객체가 있는 그리드 셀보다는 없는 셀이 훨씬 많으므로)</li>
</ul>

<h2 id="yolo의-inference">YOLO의 Inference</h2>

<p>추론 단계에서 하나의 객체를 여러 그리드 셀이 동시에 검출하는 경우가 발생하기 떄문에 non-maximal suppression 방법을 적용합니다. non-maximal suppression이란 가장 confidence가 높은 bounding box와 IOU가 일정 이상(보통 0.5)인 bounding box는 동일한 물체를 detect했다고 판단하여 지우는 작업(confidence score를 0으로 바꿈)입니다. 코드를 보면 더 이해하기 쉽습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">nms</span><span class="p">(</span><span class="n">boxes</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="s">"""Non-Maximum supression.
    Args:
    boxes: array of [cx, cy, w, h] (center format)
    probs: array of probabilities
    threshold: two boxes are considered overlapping if their IOU is largher than
        this threshold
    form: 'center' or 'diagonal'
    Returns:
    keep: array of True or False.
    """</span>
    
    <span class="n">order</span> <span class="o">=</span> <span class="n">probs</span><span class="p">.</span><span class="n">argsort</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">keep</span> <span class="o">=</span> <span class="p">[</span><span class="bp">True</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">order</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">order</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">ovps</span> <span class="o">=</span> <span class="n">batch_iou</span><span class="p">(</span><span class="n">boxes</span><span class="p">[</span><span class="n">order</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:]],</span> <span class="n">boxes</span><span class="p">[</span><span class="n">order</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">ov</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ovps</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">ov</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
        <span class="n">keep</span><span class="p">[</span><span class="n">order</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">return</span> <span class="n">keep</span>

<span class="c1">#출처: https://github.com/BichenWuUCB/squeezeDet/blob/master/src/utils/util.py
</span></code></pre></div></div>

<hr />

<p>$Reference.$</p>
<ul>
  <li><a href="https://viso.ai/deep-learning/object-detection/">https://viso.ai/deep-learning/object-detection/</a></li>
  <li><a href="https://nuggy875.tistory.com/21">https://nuggy875.tistory.com/21</a></li>
  <li><a href="https://medium.com/zylapp/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852">https://medium.com/zylapp/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852</a></li>
  <li><a href="https://arxiv.org/pdf/1908.03673.pdf">https://arxiv.org/pdf/1908.03673.pdf</a></li>
  <li><a href="https://89douner.tistory.com/88">https://89douner.tistory.com/88</a></li>
  <li><a href="https://deepsense.ai/region-of-interest-pooling-explained/">https://deepsense.ai/region-of-interest-pooling-explained/</a></li>
  <li><a href="https://ganghee-lee.tistory.com/33">https://ganghee-lee.tistory.com/33</a></li>
  <li><a href="https://bkshin.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-YOLOYou-Only-Look-Once">https://bkshin.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-YOLOYou-Only-Look-Once</a></li>
  <li><a href="https://arxiv.org/pdf/2104.11892.pdf">https://arxiv.org/pdf/2104.11892.pdf</a></li>
  <li><a href="https://www.researchgate.net/publication/335895380_Cucumber_Fruits_Detection_in_Greenhouses_Based_on_Instance_Segmentation">https://www.researchgate.net/publication/335895380_Cucumber_Fruits_Detection_in_Greenhouses_Based_on_Instance_Segmentation</a></li>
</ul>
:ET