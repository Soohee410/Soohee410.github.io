---
layout: post
title: "ë„¤ì´ë²„ ì‡¼í•‘ ë°ì´í„°ë¥¼ ì´ìš©í•œ Multi-label Classification (feat. GRU)"
date:   2021-09-10
image: '/assets/img/multilabel3.png'
tags: [Text Analysis]
use_math: false
---


# Multi-label Classificationì— ëŒ€í•˜ì—¬

ì–´ë–¤ ìƒí’ˆ ë¦¬ë·°ì— ëŒ€í•´ ëŒ€í‘œì ìœ¼ë¡œ í•  ìˆ˜ ìˆëŠ” ë¶„ì„ì€, ë¦¬ë·° ë‚´ìš©ì´ ê¸ì •ì ì¸ì§€ ë¶€ì •ì ì¸ì§€ íŒë‹¨(Sentiment Analysis)í•˜ëŠ” ê²ƒì´ ìˆì„í…ë°ìš”. ì €ëŠ” í…ìŠ¤íŠ¸ ë¶„ì„ì„ ì—°ìŠµí•˜ë˜ ì¤‘, ê°ì„± ë¶„ì„ë§Œ í•˜ê³  ëë‚˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, íŠ¹íˆ ì–´ë–¤ ë¶€ë¶„ ë•Œë¬¸ì— ë¶€ì •ì ì¸ì§€, ê¸ì •ì ì¸ì§€ê¹Œì§€ ë‚˜ì•„ê°€ë³´ê³  ì‹¶ì—ˆìŠµë‹ˆë‹¤. ì¦‰, ê°™ì€ ë¶€ì •ì ì¸ ë¦¬ë·°ë”ë¼ë„, ìƒí’ˆì˜ ``í’ˆì§ˆ``ì´ ë§ˆìŒì— ì•ˆë“¤ì—ˆëŠ”ì§€, ``ë°°ì†¡ ë° í¬ì¥`` ìƒíƒœê°€ ë§ˆìŒì— ì•ˆë“¤ì—ˆëŠ”ì§€ ì—¬ëŸ¬ê°€ì§€ ì›ì¸ì´ ìˆì„ê²ë‹ˆë‹¤. ì´ë ‡ê²Œ ìš”ì†Œë¥¼ íŒŒì•…í•˜ëŠ” ë¬¸ì œì™€ ê´€ë ¨í•˜ì—¬, ë¬¸ì¥ì—ì„œ ì£¼ìš” ìš”ì†Œë¥¼ ì¶”ì¶œ(Aspect Extraction)í•˜ëŠ” ë¹„ì§€ë„ í•™ìŠµ ë°©ë²•ë„ ì¡´ì¬í•˜ì˜€ì§€ë§Œ, ê¹”ë”í•˜ì§€ ì•Šì€ ì‹¤ì œ ê³ ê° ë¦¬ë·° ë°ì´í„°ì—ì„œ ë¹„ì§€ë„ í•™ìŠµì„ í†µí•´ ê´œì°®ì€ ì„±ëŠ¥ì„ ë‹¹ì¥ ì–»ê¸°ì—ëŠ” ì–´ë ¤ìš¸ ê²ƒìœ¼ë¡œ íŒë‹¨í–ˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ, ë¦¬ë·° ë‚´ìš©ë“¤ì„ ì°¸ê³ í•˜ì—¬, 4ê°€ì§€ í•­ëª©(**í’ˆì§ˆ ë° ì„±ëŠ¥**, **ê°€ê²© ë° í˜œíƒ**, **ë°°ì†¡ ë° ì„œë¹„ìŠ¤**, **ë””ìì¸ ë° ì™¸ê´€**)ìœ¼ë¡œ í•­ëª© ë²”ì£¼ë¥¼ ë¶„ë¥˜í•˜ì—¬, ì§€ë„ í•™ìŠµ ë°©ë²•ì„ ì´ìš©í•˜ê³ ì í–ˆìŠµë‹ˆë‹¤. ì¦‰, í•œ ë¦¬ë·° ë‚´ìš©ìœ¼ë¡œë¶€í„°, <u>ê°ì •(ê¸ì •/ë¶€ì •)</u>ê³¼ <u>ê·¸ ê°ì •ê³¼ ê´€ë ¨í•œ ìš”ì†Œ</u>ë¥¼ ë™ì‹œì— ë¶„ë¥˜(Multi-output Classification)í•˜ê³ ì í•˜ì˜€ìŠµë‹ˆë‹¤.
<br>

ì, ê·¸ëŸ°ë° ì—¬ê¸°ì„œ ìƒê°í•´ë³¼ ì ì€, í•œ ë¦¬ë·°ê°€ ë¬´ì¡°ê±´ í•˜ë‚˜ì˜ ìš”ì†Œë§Œ ì–¸ê¸‰í•˜ê³  ìˆì„ê¹Œìš”? í•­ìƒ ê·¸ë ‡ì§€ëŠ” ì•Šì„ê²ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒì˜ ë¬¸ì¥ì€,

> íŠ¼íŠ¼í•˜ë‹ˆ ì¢‹ë„¤ìš”.

4ê°€ì§€ ìš”ì†Œ ì¤‘ **í’ˆì§ˆ ë° ì„±ëŠ¥**ë§Œì„ ì–¸ê¸‰í•˜ê³  ìˆë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ°ë°, ë‹¤ìŒì˜ ë¬¸ì¥ì´ë¼ë©´,

> ê°€ê²©ë„ ì €ë ´í•˜ê³  íŠ¼íŠ¼í•˜ê³  ì¢‹ì•„ìš”.

**í’ˆì§ˆ ë° ì„±ëŠ¥** ë¿ë§Œ ì•„ë‹ˆë¼, **ê°€ê²© ë° í˜œíƒ**ë„ ì–¸ê¸‰í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì¦‰, ë¦¬ë·°ë§ˆë‹¤, ì–¸ê¸‰í•˜ê³  ìˆëŠ” ìš”ì†Œ ê°œìˆ˜ê°€ ë‹¤ë¥¸ ìƒí™©ì…ë‹ˆë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì—, 4ê°€ì§€ ìš”ì†Œ ì¤‘ í•œê°€ì§€ë¡œ ë¶„ë¥˜í•˜ëŠ” ê¸°ë³¸ **Multi-class** ë¶„ë¥˜ì™€ëŠ” ë‹¤ë¥¸ ì ‘ê·¼ì´ í•„ìš”í• í…ë°ìš”. ë°”ë¡œ, ìš”ì†Œë“¤ ê°ê°ì„ ëª¨ë‘ targetìœ¼ë¡œ í•˜ëŠ” ``Multi-label Classification``ì…ë‹ˆë‹¤! 4ë…„ ì „ ìºê¸€ì—ì„œ ì—´ë¦° [ì•…ì„± ëŒ“ê¸€ ë¶„ë¥˜ ëŒ€íšŒ](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)ê°€ Multi-label ë¶„ë¥˜ ëŒ€íšŒì˜€ëŠ”ë°ìš”. íŠ¹ì • ì•…ì„± ëŒ“ê¸€ì— ëŒ€í•´ ì™¸ì„¤ì ì¸ ë‚´ìš©ì´ í¬í•¨ë˜ë©´ 1 ì•„ë‹ˆë©´ 0, ìš•ì„¤ì´ í¬í•¨ë˜ë©´ 1 ì•„ë‹ˆë©´ 0 ë“±, ë¶„ë¥˜í•´ì•¼ í•  íƒ€ê²Ÿì´ ë³µìˆ˜ì¸ ëŒ€íšŒì˜€ìŠµë‹ˆë‹¤. ì¦‰, 4ê°€ì§€ ìš”ì†Œ ê°ê°ì— ëŒ€í•´, í•´ë‹¹ ë‚´ìš©ì´ í¬í•¨ë˜ì—ˆìœ¼ë©´ 1, ì•„ë‹ˆë©´ 0ìœ¼ë¡œ Multi-label ë¶„ë¥˜í•˜ê³ ì í•©ë‹ˆë‹¤. ë§ë¡œ ì„¤ëª…í•˜ëŠë¼ ê¸¸ì–´ì¡ŒëŠ”ë°, ì•„ë˜ì—ì„œ ë°ì´í„°ë¡œ ë³´ë©´ ë°”ë¡œ ì´í•´ë˜ì‹¤ê±°ì—ìš”! ë³¸ê²©ì ìœ¼ë¡œ ë¶„ì„ ì‹œì‘í•©ë‹ˆë‹¤!

<br>

---

# ë°ì´í„° êµ¬ì¶• ë° ì „ì²˜ë¦¬

ë°ì´í„°ë¡œëŠ”, ê¸ì •/ë¶€ì • ë¼ë²¨ë§ì´ ë˜ì–´ ìˆëŠ” [ë„¤ì´ë²„ ì‡¼í•‘ ê³ ê° ë¦¬ë·° ë°ì´í„°](https://github.com/bab2min/corpus/tree/master/sentiment)ë¥¼ ì´ìš©í–ˆìŠµë‹ˆë‹¤.

<img src="/assets/img/multilabel1.JPG" width="600px">


ì´ì œ ì¶”ê°€ì ìœ¼ë¡œ 4ê°€ì§€ í•­ëª©(í’ˆì§ˆ ë° ì„±ëŠ¥, ê°€ê²© ë° í˜œíƒ, ë°°ì†¡ ë° ì„œë¹„ìŠ¤, ë””ìì¸ ë° ì™¸ê´€)ì— ëŒ€í•´ ë¼ë²¨ë§ì„ ì§ì ‘ ì§„í–‰í•´ì•¼í–ˆìŠµë‹ˆë‹¤. 20ë§Œ ê°œì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ë¦¬ë·° ë‚´ìš©ì„ ì§ì ‘ ìˆ˜ê¸°ë¡œ ë¼ë²¨ë§í•˜ê¸°ì—ëŠ” ì‹œê°„ì´ ë¶€ì¡±í–ˆê¸° ë•Œë¬¸ì—, ë‹¤ìŒì˜ ê³¼ì •ë“¤ì„ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ ë¼ë²¨ë§ì„ ì§„í–‰í•˜ê³ ì í–ˆìŠµë‹ˆë‹¤. ë¨¼ì € ë„ì–´ì“°ê¸° ë° í† í°í™” ë“±ì˜ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•œ í›„, ì£¼ìš” í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ë¡œ 1ì°¨ ë¼ë²¨ë§ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ì´í›„ í•­ëª©ë“¤ ê°ê° í•˜ë‚˜ì”©ë§Œ ë¼ë²¨ë§ëœ ë¦¬ë·°ë“¤ë¡œ ê°„ë‹¨í•œ ëª¨ë¸ë§ì„ í†µí•´ 2ì°¨ ë¼ë²¨ë§ì„ ì§„í–‰í–ˆê³ , ìµœì¢…ì ìœ¼ë¡œ ëŒ€ëµì ìœ¼ë¡œ ê²€í† í•˜ëŠ” ì‹œê°„ì„ ê°€ì§„ í›„ ë°ì´í„° êµ¬ì¶•ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ì•„ë¬´ë˜ë„, ìˆ˜ê¸°ë¡œ ë¼ë²¨ë§í•˜ëŠ” ê²ƒì— ë¹„í•´ ì •í™•ë„ê°€ ë–¨ì–´ì§„ë‹¤ëŠ” í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤ã… ã… 

<img src="/assets/img/multilabel2.JPG" width="750px">

ì¦‰, ë¶„ë¥˜í•´ì•¼ í•  label ê°œìˆ˜ëŠ” ì´ 5ê°€ì§€ê°€ ë©ë‹ˆë‹¤!

- **sentiment**: ê¸ì •(1), ë¶€ì •(0)  
- **quality/performance**: í’ˆì§ˆ ë° ì„±ëŠ¥ì— ëŒ€í•œ ë‚´ìš© í¬í•¨(1), ë¹„í¬í•¨(0)  
- **price/event**: ê°€ê²© ë° í˜œíƒì— ëŒ€í•œ ë‚´ìš© í¬í•¨(1), ë¹„í¬í•¨(0)  
- **delivery/service**: ë°°ì†¡ ë° ì„œë¹„ìŠ¤ì— ëŒ€í•œ ë‚´ìš© í¬í•¨(1), ë¹„í¬í•¨(0)  
- **design/appearance**: ë””ìì¸ ë° ì™¸ê´€ì— ëŒ€í•œ ë‚´ìš© í¬í•¨(1), ë¹„í¬í•¨(0)


### ì „ì²˜ë¦¬

ì´ì œ ì „ì²˜ë¦¬ ê³¼ì •ì…ë‹ˆë‹¤. ì „ì²˜ë¦¬ ê³¼ì •ì€, [ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸](https://wikidocs.net/44249)ì„ ë§ì´ ì°¸ê³ í•˜ì˜€ìŠµë‹ˆë‹¤ğŸ™ ìš°ì„ , ë¦¬ë·° ë‚´ìš©ì— ëŒ€í•´, ì •ê·œ í‘œí˜„ì‹ ìˆ˜í–‰ ë° ë¹„ì–´ ìˆê±°ë‚˜ ì¤‘ë³µëœ í–‰ì„ ì œê±°í•©ë‹ˆë‹¤.

```python
df['text'] = df['text'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","")  #ì •ê·œ í‘œí˜„ì‹ ìˆ˜í–‰
df['text'].replace('', np.nan, inplace=True)  #ë¹„ì–´ ìˆëŠ” í–‰ì€ nullê°’ìœ¼ë¡œ ì²˜ë¦¬
df.dropna(how='any', inplace=True)  #null ê°’ ì œê±°
df.drop_duplicates(subset = ['text'], inplace=True) #ì¤‘ë³µëœ í–‰ ì œê±°
```
<br>

ì´ì œ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•´ì•¼ í•˜ëŠ”ë°ìš”. í† í°í™” íŒ¨í‚¤ì§€ë¡œëŠ”, [customized KoNLPy](https://github.com/lovit/customized_konlpy)ë¥¼ ì´ìš©í–ˆìŠµë‹ˆë‹¤. Customized KoNLPyì—ì„œëŠ”, ì¼ë¶€ í† í°ë“¤ì„ ì§ì ‘ ì§€ì •í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì´ ì¡´ì¬í•˜ëŠ”ë°, ì´ëŠ” ë¹ˆë²ˆí•˜ê²Œ ì‚¬ìš©ë˜ëŠ” ìœ í–‰ì–´ ë“±ì„ ì§€ì •í•  ë•Œ ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 'ê°€ì„±ë¹„'ë¼ëŠ” ë‹¨ì–´ëŠ” 'ê°€ê²© ëŒ€ë¹„ ì„±ëŠ¥'ì„ ì˜ë¯¸í•˜ëŠ”ë°, ì‡¼í•‘ ë¦¬ë·°ì—ì„œ ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ, ì¼ë°˜ KoNLPy í’ˆì‚¬ íƒœê¹…ìœ¼ë¡œ í† í°í™” ì§„í–‰í•  ê²½ìš°, 'ê°€ì„±ë¹„'ë¥¼ 'ê°€', 'ì„±ë¹„'ë¡œ ë¶„ë¦¬í•˜ëŠ” ëª¨ìŠµì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ì™€ ê°™ì´, ì‡¼í•‘ ë¦¬ë·°ì—ì„œ ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•˜ëŠ” ìœ í–‰ì–´(?)ì§€ë§Œ, ì˜ ëª» í† í°í™”ë  ê°€ëŠ¥ì„±ì´ ë†’ì€ ì¼ë¶€ ë‹¨ì–´ë“¤ì„, customized KoNLPyë¥¼ í†µí•´ í† í°ìœ¼ë¡œ ì§ì ‘ ì§€ì •í•´ì£¼ì—ˆìŠµë‹ˆë‹¤.

```python
from ckonlpy.tag import Twitter

twi = Twitter()

words = [('ê°•ì¶”','Noun'), ('ë¹„ì¶”','Noun'), ('ê°€ì„±ë¹„','Noun'),
         ('ì¬êµ¬ë§¤','Noun'), ('ì—‰ì„±', 'Noun'), ('í•', 'Noun'), ('íƒ€ì´íŠ¸','Noun')]

for word in words:
    name, poomsa = word
    twi.add_dictionary(name, poomsa)
```
<br>

ì´í›„, ë¦¬ë·°ë“¤ì— ëŒ€í•´ í† í°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ì•„ë˜ë¶€í„° ì§„í–‰ë˜ëŠ” ì½”ë“œì— ëŒ€í•œ ì„¤ëª…ì€ [ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸](https://wikidocs.net/44249)ì— ìì„¸íˆ ë‚˜ì™€ìˆê¸° ë•Œë¬¸ì—, ì„¤ëª…ì€ ìƒëµí•˜ê² ìŠµë‹ˆë‹¤!

```python
stopwords = ['ì˜', 'ê°€', 'ì´', 'ì€', 'ë“¤', 'ëŠ”', 'ê³¼', 'ë„', 'ë¥¼', 'ìœ¼ë¡œ', 'ì—',
             'í•˜ë‹¤', 'ì„', 'ì´ë‹¤', 'ê²ƒ', 'ë¡œ', 'ì—ì„œ', 'ê·¸', 'ì¸', 'ì„œ', 'ë„¤ìš”',
             'ì„', 'ë‘', 'ê²Œ', 'ìš”', 'ì—ê²Œ', 'ì—”']

text_token = []
for sentence in df['text']:
    tmp = []
    tmp = twi.morphs(sentence, stem=True, norm=True)  #í† í°í™”
    tmp = [word for word in tmp if not word in stopwords]  #ë¶ˆìš©ì–´ ì œê±°
    text_token.append(tmp)
```
<br>

```python
from keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(text_token)
```
<br>

```python
threshold = 3
total_cnt = len(tokenizer.word_index) # ë‹¨ì–´ì˜ ìˆ˜
rare_cnt = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì¹´ìš´íŠ¸
total_freq = 0 # í›ˆë ¨ ë°ì´í„°ì˜ ì „ì²´ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ì´ í•©
rare_freq = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ì˜ ì´ í•©

# ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ì˜ ìŒ(pair)ì„ keyì™€ valueë¡œ ë°›ëŠ”ë‹¤.
for key, value in tokenizer.word_counts.items():
    total_freq = total_freq + value

    # ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ìœ¼ë©´
    if(value < threshold):
        rare_cnt = rare_cnt + 1
        rare_freq = rare_freq + value

print('ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° :',total_cnt)
print('ë“±ì¥ ë¹ˆë„ê°€ %së²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: %s'%(threshold - 1, rare_cnt))
print("ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨:", (rare_cnt / total_cnt)*100)
print("ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨:", (rare_freq / total_freq)*100)
```

```
ë‹¨ì–´ ì§‘í•©(vocabulary)ì˜ í¬ê¸° : 44152
ë“±ì¥ ë¹ˆë„ê°€ 2ë²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: 26822
ë‹¨ì–´ ì§‘í•©ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨: 60.74922993295887
ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨: 1.2691737130692322
```
<br>

```python
# ë¹ˆë„ìˆ˜ 3ì´í•˜ì¸ ë‹¨ì–´ ì œê±°í•˜ê³  0 ì¶”ê°€í•œ ê°œìˆ˜ = vocab_size
vocab_size = total_cnt - rare_cnt + 1
print('ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° :',vocab_size)
```
<br>

```
ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸° : 17331
```
<br>

ì „ì²´ì—ì„œ ë“±ì¥ ë¹ˆë„ ìˆ˜ê°€ 3 ì´í•˜ì¸ ë‹¨ì–´ë“¤ì€ ì œì™¸í•˜ì—¬, (0ì„ í¬í•¨í•œ) ì „ì²´ **ë‹¨ì–´ ì§‘í•©(vocab_size)**ì˜ í¬ê¸°ëŠ” 17331ê°œë¡œ í•©ë‹ˆë‹¤. ì´ì œ ë¬¸ì¥ì„ íŒ¨ë”©í•˜ê¸° ìœ„í•´, ë¬¸ì¥ ê¸¸ì´ë¥¼ ì–´ëŠì •ë„ë¡œ ê³ ì •í• ì§€ ì •í•©ë‹ˆë‹¤.

```python
tokenizer = Tokenizer(num_words = vocab_size)
tokenizer.fit_on_texts(text_token)

X = tokenizer.texts_to_sequences(text_token)
print('ë¦¬ë·°ì˜ ìµœëŒ€ ê¸¸ì´ :',max(len(l) for l in X))
print('ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ :',sum(map(len, X))/len(X))
```
<br>

```
ë¦¬ë·°ì˜ ìµœëŒ€ ê¸¸ì´ : 58
ë¦¬ë·°ì˜ í‰ê·  ê¸¸ì´ : 12.36651295354987
```
<br>

```python
def below_threshold_len(max_len, nested_list):
    cnt = 0
    for s in nested_list:
        if(len(s) <= max_len):
            cnt = cnt + 1
    print('ì „ì²´ ìƒ˜í”Œ ì¤‘ ê¸¸ì´ê°€ %s ì´í•˜ì¸ ìƒ˜í”Œì˜ ë¹„ìœ¨: %s'%(max_len, (cnt / len(nested_list))*100))

max_len = 35
below_threshold_len(max_len, X)
```
<br>

```
ì „ì²´ ìƒ˜í”Œ ì¤‘ ê¸¸ì´ê°€ 35 ì´í•˜ì¸ ìƒ˜í”Œì˜ ë¹„ìœ¨: 97.29337795864298
```
<br>

ë¬¸ì¥ ê¸¸ì´ê°€ 35ë¡œ í•  ê²½ìš°, ì•½ 97%ì˜ ëŒ€ë¶€ë¶„ì˜ ìƒ˜í”Œì„ ì»¤ë²„ ê°€ëŠ¥í•©ë‹ˆë‹¤. **ë¬¸ì¥ ê¸¸ì´(max_len)**ëŠ” 35ë¡œ ê²°ì •í•©ë‹ˆë‹¤. ì´ì œ, ê° ë¦¬ë·° ìƒ˜í”Œë“¤ì— ëŒ€í•´ íŒ¨ë”©ì„ ì§„í–‰í•©ë‹ˆë‹¤.

```python
from tensorflow.keras.preprocessing.sequence import pad_sequences

X = pad_sequences(X, maxlen = max_len)
X
```
<br>

```
array([[    0,     0,     0, ..., 11842,    25,   273],
       [    0,     0,     0, ...,   155,  1599,  1868],
       [    0,     0,     0, ...,     5,   169,   245],
       ...,
       [    0,     0,     0, ...,   101,   372,   421],
       [    0,     0,     0, ...,  1688,    17,   323],
       [    0,     0,     0, ...,    76,     3,   205]])
```
<br>

í•œí¸, targetì— í•´ë‹¹í•˜ëŠ” yì—ëŠ” 5ê°œì˜ label(sentiment, quality/performance, price/event, delivery/service, design/appearance)ì´ ë“¤ì–´ê°€ê²Œ ë©ë‹ˆë‹¤.

```python
y = np.array(df.iloc[:,:5])
y
```
<br>

```
array([[1, 0, 0, 1, 0],
       [0, 0, 0, 1, 0],
       [1, 1, 1, 0, 0],
       ...,
       [1, 0, 1, 0, 0],
       [1, 0, 0, 0, 1],
       [1, 0, 0, 1, 0]], dtype=int64)
```
<br>

ì, ì´ë¡œì¨ í•™ìŠµì— ì´ìš©í•  X, targetì¸ yë¥¼ ëª¨ë‘ êµ¬ì¶•í–ˆê³ , ëª¨ë¸ë§ì„ ìœ„í•´ train, test set ë¶„ë¦¬ë¥¼ í•©ë‹ˆë‹¤.

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)
X_train = np.array(X_train)
X_test = np.array(X_test)
```

<br>

---

# ëª¨ë¸ë§

ë°ì´í„° êµ¬ì¶•ê³¼ ì „ì²˜ë¦¬ë¥¼ ëª¨ë‘ ì™„ë£Œí–ˆê³ , ëª¨ë¸ë§ì„ ì§„í–‰í•  ì°¨ë¡€ì…ë‹ˆë‹¤! ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ``GRU``ë¥¼ ì´ìš©í•œ Multi-label ì•„í‚¤í…ì²˜ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ë§ì„ ì§„í–‰í•˜ê³ ì í•©ë‹ˆë‹¤. ì¦‰, ê° 5ê°œì˜ ì´í•­ ë¶„ë¥˜ë¥¼ ë™ì‹œì— ì§„í–‰í•˜ëŠ”, ``Multi-output Binary Classification`` ë¬¸ì œì…ë‹ˆë‹¤.

<img src="/assets/img/multilabel3.png" width="750px">

ê°ê°ì˜ ì´í•­ ë¶„ë¥˜ì— ìˆì–´, ì§€í‘œëŠ” **F1-score**ë¡œ ê²°ì •í–ˆìŠµë‹ˆë‹¤. ëŒ€ì²´ë¡œ, í’ˆì§ˆ ë° ì„±ëŠ¥ì— ëŒ€í•œ ì–¸ê¸‰ì€ ìƒë‹¹ ìˆ˜ê°€ í¬í•¨í•˜ê³  ìˆëŠ” ê²ƒì— ë¹„í•´, ê°€ê²© ë“±ì˜ ê·¸ ì™¸ ìš”ì†Œë“¤ì— ëŒ€í•´ì„œëŠ” ë¹„í¬í•¨ ë¹„ìœ¨ì´ í›¨ì”¬ ë†’ì•„, ë¹„ìœ¨ ì°¨ì´ê°€ ë¶ˆê· í˜•í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

```python
import tensorflow.keras.backend as K

def F1score(y_true, y_pred):
    eps = K.epsilon()
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    real_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    pred_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    recall = true_positives / (real_positives + eps)
    precision = true_positives / (pred_positives + eps)
    f1_score = 2 * (recall * precision) / (recall + precision + eps)
    return f1_score
```
<br>

ë³¸ê²©ì ìœ¼ë¡œ fití•˜ê¸° ì „ì—, multi-output êµ¬ì¡°ì— ë§ê²Œ ë‹¤ìŒê³¼ ê°™ì´ yë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€ê²½í•´ì¤ë‹ˆë‹¤.

```python
label_names = df.columns[:5] #label ì¢…ë¥˜
n_label = len(label_names)

y_train_list = {label_names[i]:y_train[:,i] for i in range(n_label)}
y_test_list = {label_names[i]:y_test[:,i] for i in range(n_label)}

y_train_list
```
<br>

```
{'sentiment': array([1, 1, 1, ..., 0, 0, 0], dtype=int64),
 'quality/performance': array([0, 1, 0, ..., 1, 1, 1], dtype=int64),
 'price/event': array([0, 0, 0, ..., 0, 0, 0], dtype=int64),
 'delivery/service': array([0, 0, 1, ..., 0, 0, 0], dtype=int64),
 'design/appearance': array([1, 1, 0, ..., 0, 0, 0], dtype=int64)}
```
<br>

ì´ì œ ëª¨ë¸ ì‹ì„ ì„¸ì›Œì¤ë‹ˆë‹¤!

```python
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, Embedding, Dense, GRU, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

def SimpleGRU():
    input_layer = Input(shape=(max_len,))
    embedding_text = Embedding(vocab_size, 256)(input_layer)
    x = GRU(128)(embedding_text)
    x = Dropout(0.5)(x)
    output_layers = [Dense(1, activation="sigmoid", name=label_names[i])(x) for i in range(y.shape[1])]

    model = Model(inputs=input_layer, outputs=output_layers)
    model.compile(loss=['binary_crossentropy']*n_label, optimizer='rmsprop', metrics=[F1score])

    return model


model = SimpleGRU()
```
<br>

```python
callback_list = [EarlyStopping(monitor='val_loss', patience=4),
                ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]
history = model.fit(X_train, y_train_list, epochs=20, batch_size=60, validation_split=0.1, callbacks=callback_list)
```
<br>

ì—í­ì— ë”°ë¥¸ ê° labelë“¤ì˜ ì„±ëŠ¥ ê°’ì„ ì‹œê°í™”í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
def history_to_value(history, n_label):
    history_out = pd.DataFrame(history.history)
    epochs = history_out.shape[0]
    history_out = history_out.drop('loss', axis=1).stack().reset_index()
    history_out['F1score'] = history_out['level_1'].apply(lambda x: 1 if x.split('_')[-1] =='F1score' else 0)
    history_out['label_kind'] = history_out['level_1'].apply(lambda x: x.split('_')[-2])
    history_out['data'] = history_out['level_1'].apply(lambda x: 'validation' if len(x.split('_')) == 3 else 'train')
    history_out = history_out[history_out['F1score'] == 1][[0, 'label_kind', 'data']].rename(columns={0:'values'}).reset_index(drop=True)
    history_out.sort_values(by=['label_kind', 'data'], inplace=True)
    history_out['epochs'] = list(range(1,epochs+1)) * n_label * 2
    return history_out

def plotDF(history, n_label):
    history_out = history_to_value(history, n_label)
    fig = sns.relplot(data=history_out, x="epochs", y="values", hue="data", col="label_kind", linewidth=2.5, kind='line', col_wrap=3,
                      marker='o', palette=pal[:2], height=3, aspect=0.8)
    plt.legend(loc='lower right')
    leg = fig._legend
    leg.set_bbox_to_anchor([0.9,0.3])
    fig.set(ylim=(0.6, 1), ylabel='F1 score')

plotDF(history1, n_label)
```

<img src="/assets/img/multilabel4.JPG" width="750px">

í•™ìŠµ ê³¼ì •ì—ì„œ ì €ì¥í•œ best_modelë¡œ test setì—ì„œì˜ ê²°ê³¼ë„ í™•ì¸í•©ë‹ˆë‹¤.

```python
best_model = load_model('best_model.h5', custom_objects = {'F1score':F1score})
best_model.evaluate(X_test, y_test_list, batch_size=60)
```
<br>

```
665/665 [==============================] - 5s 7ms/step - loss: 0.4103 - sentiment_loss: 0.2328 - quality/performance_loss: 0.0550 - price/event_loss: 0.0165 - delivery/service_loss: 0.0264 - design/appearance_loss: 0.0797 - sentiment_sentiment_F1score: 0.9115 - quality/performance_quality/performance_F1score: 0.9903 - price/event_price/event_F1score: 0.9874 - delivery/service_delivery/service_F1score: 0.9886 - design/appearance_design/appearance_F1score: 0.9455
[0.4102814495563507,
 0.23277169466018677,
 0.05498204380273819,
 0.016451003029942513,
 0.026363756507635117,
 0.07971274852752686,
 0.9114562273025513,
 0.9903451800346375,
 0.987377405166626,
 0.9885985255241394,
 0.9454994797706604]
```

<br>

---

# Sentiment, Category ë¶„ë¥˜ í”„ë¡œê·¸ë¨

ì—¬ê¸°ê¹Œì§€ ê°„ë‹¨ ëª¨ë¸ë§ì„ ëª¨ë‘ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤! ì´ë²ˆì—ëŠ”, íŠ¹ì • ë¦¬ë·°ë¥¼ ì¸í’‹ìœ¼ë¡œ ì§‘ì–´ë„£ì—ˆì„ ë•Œ, ì•ì„œ ì–»ì€ best_modelì˜ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì‹œê°í™”í•´ì„œ ì•Œë ¤ì£¼ëŠ” í”„ë¡œê·¸ë¨ì„ ë§Œë“¤ì–´ë³´ê³ ì í•©ë‹ˆë‹¤.

```python
def predict_review(sentence, max_len=35):
    #Preprocess, Predict
    sentence = sentence.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]","") # ì •ê·œ í‘œí˜„ì‹ ìˆ˜í–‰
    if sentence == '':
        return "ë¦¬ë·° í•´ì„ ë¶ˆê°€."

    new = twi.morphs(sentence)
    new = [word for word in new if not word in stopwords]
    encoded = list(filter(lambda x: x>0, [tokenizer.word_index.get(i,0) for i in new])) #ë‹¨ì–´ë¥¼ ì¸ë±ìŠ¤ë¡œ ë°”ê¿ˆ
    pad_new = [0]*(max_len-len(encoded)) + encoded    # íŒ¨ë”©
    scores = [i[0][0] for i in best_model.predict(np.array([pad_new]))] #ì´ì „ì— í•™ìŠµí•œ ëª¨ë¸ë¡œ ì˜ˆì¸¡í•œ í›„ ê²°ê³¼ ì €ì¥.

    #Visualize
    fig, ax = plt.subplots(1,2, figsize=(9,4))

    x, y = label_names[:0:-1], scores[:0:-1]
    colors = ['#E47F2D' if yy > 0.5 else '#747B86' for yy in y]  ##í™•ë¥ ì´ 0.5 ë„˜ëŠ” ê²½ìš° ì£¼í™©ìƒ‰
    ax[0].barh(x, y, color=colors, height=0.5)
    ax[0].set_title("Category")
    ax[0].set_xlim(0,1)
    for i in range(4):
        ax[0].text(y[i]+0.01, x[i], '{:.2f}%'.format(y[i]*100), verticalalignment='center')
    simpleaxis(ax[0])

    x, y = ["ê¸ì •", "ë¶€ì •"], [scores[0], 1-scores[0]]
    colors = ['#E47F2D' if yy > 0.5 else '#747B86' for yy in y]
    ax[1].bar(x, y, width=0.25, color=colors)
    ax[1].set_title("Sentiment")
    ax[1].set_ylim(0,1)
    for i in range(2):
        ax[1].text(x[i], y[i]+0.03, '{:.2f}%'.format(y[i]*100), horizontalalignment='center')
    simpleaxis(ax[1])
```
<br>

ì´ì œ ë¬¸ì¥ì„ ì§‘ì–´ ë„£ê³  ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. "í€„ë¦¬í‹° ê´œì°®ì•„ìš”"ì™€ "í€„ë¦¬í‹° ëŒ€ë°• ì¢‹ì•„ìš”" ë‘ ë¬¸ì¥ì´ ìˆì„ ë•Œ, í›„ìì˜ ê²½ìš° ê¸ì •ì¼ í™•ë¥ ì´ ë§¤ìš° ë†’ì•„ì§„ ê²ƒì„ ë³¼ ìˆ˜ê°€ ìˆë„¤ìš”!

```python
text = "í€„ë¦¬í‹° ê´œì°®ì•„ìš”"
predict_review(text)
```
<img src="/assets/img/multilabel5.JPG" width="750px">


```python
text = "í€„ë¦¬í‹° ëŒ€ë°• ì¢‹ì•„ìš”"
predict_review(text)
```
<img src="/assets/img/multilabel6.JPG" width="750px">

ë¦¬ë·° ë‚´ìš©ì— ì–¸ê¸‰ëœ ì¹´í…Œê³ ë¦¬ê°€ 2ê°œ ì´ìƒì¼ ë•Œì—ë„, ë‹¤ìŒê³¼ ê°™ì´ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
text = "ì œí’ˆ ì‹¤ë°¥ë„ ë§ì´ í’€ë ¤ìˆê³  í—ˆì ‘í•©ë‹ˆë‹¤ í¬ê¸°ë„ ëŒ€í˜•ì¸ê°€ ì‹¶ì„ ì •ë„ë¡œ í¬ì§„ ì•Šë„¤ìš”"
predict_review(text)
```
<img src="/assets/img/multilabel7.JPG" width="750px">

<br>

---

# í•œê³„ì 

ì´ë ‡ê²Œ í•´ì„œ Multi-label Classificationì„ ì§„í–‰í•´ë³´ì•˜ìŠµë‹ˆë‹¤! í•˜ì§€ë§Œ, ì¼ë¶€ í•œê³„ì ë“¤ì´ ìˆëŠ”ë°ìš”.

- **ë°ì´í„° ë¼ë²¨ë§ì˜ ì •í™•ë„ ë¶€ì¡±**  
ìˆ˜ê¸°ë¡œ ì§ì ‘ ë¼ë²¨ë§í•œ ê²ƒì´ ì•„ë‹ˆê¸°ì—, ë¼ë²¨ë§ ì •í™•ë„ê°€ í˜„ì €íˆ ë¶€ì¡±í•˜ì˜€ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ì´ëŠ” ì •í™•í•˜ê²Œ ë¼ë²¨ë§ì´ ë˜ì–´ ìˆëŠ” ë‹¤ë¥¸ ë°ì´í„°ê°€ ì¡´ì¬í•  ê²½ìš° ë¬¸ì œê°€ ë˜ëŠ” ë¶€ë¶„ì€ ì•„ë‹™ë‹ˆë‹¤.  

- **Label ê°„ ìƒí˜¸ ì˜ì¡´ì„± ë¬´ì‹œ**  
í•´ë‹¹ í¬ìŠ¤íŠ¸ì—ì„œëŠ”, 4ê°€ì§€ í•­ëª©(í’ˆì§ˆ, ê°€ê²©, ë°°ì†¡, ë””ìì¸) ê°ê°ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ì´í•­ ë¶„ë¥˜ë¥¼ ì§„í–‰í–ˆëŠ”ë°ìš”. ì´ë ‡ê²Œ í•˜ëŠ” ê²ƒì€, ê° 4ê°€ì§€ labelì´ ì„œë¡œ ë…ë¦½ì  ê´€ê³„ë¼ëŠ” ì „ì œë¥¼ ê¹”ê³  ê°€ê²Œ ë©ë‹ˆë‹¤. ìƒí’ˆ ë˜ëŠ” ì‡¼í•‘ ê²½í—˜ì— ëŒ€í•œ ì—¬ëŸ¬ê°€ì§€ ì¹´í…Œê³ ë¦¬ë“¤ì´ ì™„ì „íˆ ìƒí˜¸ ë…ë¦½ì  ê´€ê³„ë¼ê³  ë³´ê¸° ì–´ë µê¸° ë•Œë¬¸ì—, ì´ë“¤ì˜ ê´€ê³„ë¥¼ ë¬´ì‹œí•˜ê³  ê°ê° ì´í•­ ë¶„ë¥˜ë¥¼ í•˜ëŠ” ê²ƒì€ ì—„ë°€í•˜ê²ŒëŠ” í—ˆì ì´ ì¡´ì¬í•©ë‹ˆë‹¤. [Deep dive into multi-label classification..! (With detailed Case Study)](https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff)ë¥¼ ì°¸ê³ í•˜ë©´, ëª¨ë“  ë¶„ë¥˜ ì¡°í•©ì— ëŒ€í•´ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•, ë˜ëŠ” ì´ì „ labelì— ëŒ€í•œ ë¶„ë¥˜ê¸° ì˜ˆì¸¡ê°’ì„ ê·¸ ë‹¤ìŒ label ë¶„ë¥˜ì— ë³€ìˆ˜ë¡œì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì¸ Classifier Chains ë“± Multi-label ë¬¸ì œë¥¼ í‘¸ëŠ” ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ì´ ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ì´ë“¤ì˜ ì˜ì¡´ ê´€ê³„ë¥¼ ë°˜ì˜í•  ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ ë°©ë²•ì„ ì‹œë„í•  í•„ìš”ì„±ì´ ìˆìŠµë‹ˆë‹¤.



<br>

---

### Reference

- [ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸](https://wikidocs.net/44249)   
- [customized KoNLPy](https://github.com/lovit/customized_konlpy)  
- [Deep dive into multi-label classification..! (With detailed Case Study)](https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff)

<br>
